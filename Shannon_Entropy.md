香农熵 （Shannon Entropy）

$$
H = - \sum_{i=1}^n p_i \log_2 p_i
$$

其实是**信息论里衡量“随机性/不确定性”的一个指标**。

---

### 1. 为什么有一个负号

* 概率 $p_i$ 都在 0\~1 之间，所以 $\log_2 p_i$ 是**非正数**（小于等于 0）。
* 为了让熵 $H$ 是一个**非负数**（信息量不能是负的），我们前面加一个负号。

例子：
$p=0.5$ 时，$\log_2 0.5 = -1$，所以 $-p\log_2 p = 0.5 \times 1 = 0.5$。

---

### 2. 物理意义 / 直观解释

* $p_i$ 是事件 $i$ 发生的概率。
* $\log_2 \frac{1}{p_i}$ 表示事件发生时的信息量（单位：bit），概率越小，信息量越大。
* 熵就是**信息量的期望值**（平均信息量）：

  $$
  H = \sum p_i \times \log_2 \frac{1}{p_i}
  $$

  也就是 $-\sum p_i \log_2 p_i$。

---

### 3. 举个简单例子

1. **均匀分布** $p = [0.5, 0.5]$：

   $$
   H = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \ \text{bit}
   $$

   —— 最大不确定性。

2. **极端分布** $p = [1, 0]$：

   $$
   H = -(1\log_2 1 + 0\log_2 0) = 0
   $$

   —— 没有不确定性（事件必然发生）。

---

### 4. 编程时的注意

* 要避免 $p_i = 0$ 时 $\log_2 0$ 出现 NaN，一般会写：

  ```matlab
  H = -sum(p .* log2(p + eps));
  ```
* 这里 `eps` 是一个很小的数，防止 log(0)。

---

